{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2 \n",
    "from tacobox import Taco\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import RobertaTokenizerFast, GPT2Tokenizer\n",
    "from transformers import RobertaConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "from jiwer import cer\n",
    "from evaluate import load\n",
    "cer = load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"./data_uhwr/train.csv\")\n",
    "eval_df = pd.read_csv(r\"./data_uhwr/val.csv\")\n",
    "test_df = pd.read_csv(r'./data_uhwr/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWRDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_width = 1600, \n",
    "                 input_height = 64,\n",
    "                 aug = False,\n",
    "                 taco_aug_frac = 0.9):\n",
    "        self.df = df\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mytaco = Taco(\n",
    "            cp_vertical = 0.2,\n",
    "            cp_horizontal = 0.25,\n",
    "            max_tw_vertical = 100,\n",
    "            min_tw_vertical = 10,\n",
    "            max_tw_horizontal = 50,\n",
    "            min_tw_horizontal = 10\n",
    "        )\n",
    "        self.aug = aug\n",
    "        self.taco_aug_frac = taco_aug_frac\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        \n",
    "        image = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        pixel_values = self.preprocess(image, self.aug)\n",
    "\n",
    "        # image = cv2.resize(image, (512, 64), cv2.INTER_AREA)\n",
    "\n",
    "        # pixel_values = Image.fromarray(image.T)\n",
    "        try:\n",
    "            labels = self.tokenizer(text).input_ids\n",
    "        except:\n",
    "            labels = None\n",
    "            encoding = (None, None)\n",
    "        else:\n",
    "            labels = [self.tokenizer.bos_token_id] + [label if label != self.tokenizer.pad_token_id else -100 for label in labels]+[self.tokenizer.eos_token_id]\n",
    "            encoding = (torch.tensor(pixel_values[None,:,:]).float(), torch.tensor(labels))\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "    def preprocess(self, img, augment=True):\n",
    "        if augment:\n",
    "            img = self.apply_taco_augmentations(img)\n",
    "            \n",
    "        # scaling image [0, 1]\n",
    "        img = img/255\n",
    "        img = img.swapaxes(-2,-1)[...,::-1]\n",
    "        # img = img.swapaxes(-2,-1)\n",
    "        target = np.ones((self.input_width, self.input_height))\n",
    "        # target = np.zeros((self.input_width, self.input_height))\n",
    "        new_x = self.input_width/img.shape[0]\n",
    "        new_y = self.input_height/img.shape[1]\n",
    "        min_xy = min(new_x, new_y)\n",
    "        new_x = int(img.shape[0]*min_xy)\n",
    "        new_y = int(img.shape[1]*min_xy)\n",
    "        img2 = cv2.resize(img, (new_y,new_x))\n",
    "        target[:new_x,:new_y] = img2\n",
    "        return 1 - (target)\n",
    "        # return (target)\n",
    "        #89 1661\n",
    "\n",
    "    def apply_taco_augmentations(self, input_img):\n",
    "        random_value = random.random()\n",
    "        if random_value <= self.taco_aug_frac:\n",
    "            augmented_img = self.mytaco.apply_vertical_taco(\n",
    "                input_img, \n",
    "                corruption_type='random'\n",
    "            )\n",
    "        else:\n",
    "            augmented_img = input_img\n",
    "        return augmented_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"vocabs/ved/\") # Custom tokenizer\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.unk_token = '<unk>'\n",
    "\n",
    "\n",
    "train_dataset = HWRDataset(df = train_df, tokenizer = tokenizer, aug = True)\n",
    "eval_dataset = HWRDataset(df = eval_df, tokenizer = tokenizer)   \n",
    "print(\"Number of training examples: \", len(train_dataset))\n",
    "print(\"Number of validation examples: \", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "N = 5\n",
    "encoding = train_dataset[N]\n",
    "figure(figsize=(10, 10), dpi=80)\n",
    "\n",
    "# plt.imshow(encoding['pixel_values'].permute(1,2,0))\n",
    "plt.imshow(encoding[0].permute(1,2,0), cmap=\"gray\")\n",
    "# plt.imshow(encoding[0], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_df['file_name'][N]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = encoding['labels']\n",
    "labels = encoding[1]\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    batch_dict = {}\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        if src_sample == None:\n",
    "            continue\n",
    "        if tgt_sample == None:\n",
    "            continue\n",
    "        src_batch.append(src_sample) #ADDEDLINE\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_batch = torch.stack(src_batch)\n",
    "\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first = True, padding_value = -100)\n",
    "\n",
    "    batch_dict['pixel_values'] = src_batch\n",
    "    batch_dict['labels'] = tgt_batch\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv_transformer(vocab_size):\n",
    "\n",
    "    class Conv(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Conv, self).__init__()            # 512 * 64\n",
    "\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(2, 2)      #    256 * 32\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(2, 2)    # 128 * 16\n",
    "            )\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv2d(32, 48, 3, padding=1),\n",
    "                nn.BatchNorm2d(48),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(48, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d((1, 2), (1, 2)),   # 128 * 8\n",
    "                nn.Dropout2d(0.2),\n",
    "            )\n",
    "            self.conv4 = nn.Sequential(\n",
    "                nn.Conv2d(64, 96, 3, padding=1),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(96, 128, 3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d((1, 2), (1, 2)),    # 128 * 4\n",
    "                nn.Dropout2d(0.2),\n",
    "            )\n",
    "            self.conv5 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 4),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "            )  \n",
    "\n",
    "        def forward(self,\n",
    "                src: Tensor,\n",
    "               ):\n",
    "\n",
    "            src = self.conv1(src)\n",
    "            # print(x.shape)                                 # (*, 16, 32, 256)\n",
    "            src = self.conv2(src)\n",
    "            # print(x.shape)                                 # (*, 32, 16, 128)\n",
    "            src = self.conv3(src)\n",
    "            # print(x.shape)                                 # (*, 64, 8, 128)\n",
    "            src = self.conv4(src)\n",
    "            # print(x.shape)                                 # (*, 128, 4, 128)        \n",
    "            src = self.conv5(src)\n",
    "            # print(x.shape)                                 # (*, 256, 1, 125)\n",
    "            src = src.squeeze(-1)\n",
    "            src = src.permute((0, 2, 1)).contiguous()        # (*, 125, 256)\n",
    "\n",
    "            return src \n",
    "\n",
    "    model_conv = Conv()   \n",
    "\n",
    "    dec = {'vocab_size':vocab_size,\n",
    "           'n_positions':512,\n",
    "           'n_embd':256,\n",
    "           'n_head':4,\n",
    "           'n_layer':2\n",
    "           }\n",
    "\n",
    "    enc = {'vocab_size':vocab_size,\n",
    "           'num_hidden_layers':2,\n",
    "           'hidden_size':256,\n",
    "           'num_attention_heads':4,\n",
    "           'intermediate_size':1024,\n",
    "           'hidden_act':'gelu'\n",
    "           }\n",
    "\n",
    "    enc_config = RobertaConfig(**enc)\n",
    "    dec_config = GPT2Config(**dec)\n",
    "    \n",
    "    config = EncoderDecoderConfig.from_encoder_decoder_configs(enc_config, dec_config)\n",
    "    model_transformer = EncoderDecoderModel(config=config)\n",
    "\n",
    "    return model_conv, model_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conv, transformer = model_conv_transformer(vocab_size = tokenizer.vocab_size)\n",
    "\n",
    "conv.to(device)\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True, collate_fn = collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size = 16, collate_fn = collate_fn)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size = 16, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "transformer.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "transformer.config.vocab_size = transformer.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "transformer.config.eos_token_id = tokenizer.eos_token_id\n",
    "transformer.config.max_length = 256\n",
    "# transformer.config.max_new_tokens = 256 \n",
    "transformer.config.early_stopping = False\n",
    "transformer.config.no_repeat_ngram_size = 0\n",
    "transformer.config.length_penalty = 1\n",
    "transformer.config.num_beams = 4\n",
    "transformer.config.temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    print(pred_str[0])\n",
    "    print(label_str[0])\n",
    "\n",
    "    combine = [(x, y) for x, y in zip(pred_str, label_str) if x]\n",
    "\n",
    "    pred_str = [x for x, y in combine]\n",
    "    label_str = [y for x, y in combine]\n",
    "\n",
    "    cer_score = cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "params = list(conv.parameters()) + list(transformer.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = 0.0003, betas = (0.9, 0.98), eps = 1e-9)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_eval_steps = num_epochs * len(eval_dataloader)\n",
    "\n",
    "print(num_training_steps)\n",
    "\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_eval_steps))\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    # train\n",
    "    conv.train()\n",
    "    transformer.train()\n",
    "    train_loss = 0.0\n",
    "    eval_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "    total_train = 0.0\n",
    "    correct_eval = 0.0\n",
    "    total_eval = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = conv(batch['pixel_values'])\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = transformer(inputs_embeds=outputs, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = preds = torch.argmax(logits, axis=-1)\n",
    "            mask = torch.ones_like(labels).to(device)\n",
    "            mask[labels==-100] = 0\n",
    "            correct_train += ((preds == labels)*mask).sum()\n",
    "            total_train += mask.sum()\n",
    "\n",
    "        progress_bar_train.update(1)\n",
    "        \n",
    "    print(f\"Train Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    print(f\"Train ACC after epoch {epoch}:\", correct_train/total_train)\n",
    "    \n",
    "    # evaluate\n",
    "    conv.eval()\n",
    "    transformer.eval()\n",
    "    valid_cer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            for k,v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            outputs = conv(batch['pixel_values'])\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = transformer(inputs_embeds = outputs, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, axis=-1)\n",
    "            mask = torch.ones_like(labels).to(device)\n",
    "            mask[labels==-100] = 0\n",
    "            correct_eval += ((preds == labels)*mask).sum()\n",
    "            total_eval += mask.sum()\n",
    "\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "        print(f\"Val Loss after epoch {epoch}:\", eval_loss/len(eval_dataloader))\n",
    "        print(f\"Val ACC after epoch {epoch}:\", correct_eval/total_eval)\n",
    "\n",
    "\n",
    "#         transformer.save_pretrained(\"./conv_transformer_weights/icdar\")\n",
    "#         torch.save(conv.state_dict(), \"./conv_transformer_weights/icdar/conv.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.eval()\n",
    "transformer.eval()\n",
    "test_cer = 0.0\n",
    "data_loader = test_dataloader\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "        # run batch generation\n",
    "        outputs = conv(batch[\"pixel_values\"].to(device))\n",
    "        outputs = transformer.generate(inputs_embeds = outputs)\n",
    "        # compute metrics\n",
    "        error = compute_cer(pred_ids = outputs, label_ids = batch[\"labels\"])\n",
    "        test_cer += error \n",
    "\n",
    "print(\"Validation CER:\", test_cer / len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cer / len(test_dataloader)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a900363777843d533ef46ef4cddd42cd4196186c9a8c759e4e50dff4b09af016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
